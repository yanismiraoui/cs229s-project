# BOLT âš¡ï¸: Bash Optimization through Lightweight Transformers

[BOLT Leaderboard](https://bolt-dashboard.replit.app/)  

Efficient large language models that convert natural language descriptions into shell/bash commands, using optimization techniques such as pruning, quantization, assisted decoding and fine-tuning. This repository contains the code for training, evaluating and visualizing the results with a web leaderboard. Make your bash commands faster, smaller and more efficient! ğŸš€

## ğŸŒŸ Features

- **Quantization**: Quantize the model weights to reduce the model size and improve inference speed
- **Pruning**: Prune the model to reduce the model size
- **Assisted Decoding**: Use an ensemble of language models to improve the quality of the generated commands
- **Fine-tuning**: Fine-tune the model on a custom dataset to improve the model's performance on specific tasks
- **Command Validation**: Built-in syntax validation for generated bash commands
- **Evaluation System**: Using LLM-as-a-judge to evaluate the quality of the generated commands
- **Resource Monitoring**: CPU and memory usage tracking during model operations
- **Web Leaderboard**: Interactive interface for testing and visualizing model outputs

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- PyTorch
- Hugging Face Transformers

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yanismiraoui/cs229s-project.git
cd cs229s-project
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“š Usage

### Evaluation ğŸ§ª

To evaluate the model, run the following command after setting the parameters in the `configs/config.yaml` file:

```bash
python evaluate.py
```

Note: To use the LLM-as-a-judge, you need to have an OpenAI API key and save it in a `OPENAI_API_KEY` file in the root directory.

### Web Leaderboard ğŸ†

To launch the web interface:

```bash
python dashboard/app.py
```

## ğŸ”§ Project Structure

```
.
â”œâ”€â”€ configs/            # Configuration files
â”œâ”€â”€ dashboard/         # Web interface
â”œâ”€â”€ evaluators/        # Evaluation modules
â”œâ”€â”€ models/           # Model architecture
â”œâ”€â”€ utils/            # Utility functions
â”‚   â”œâ”€â”€ finetune.py
â”‚   â”œâ”€â”€ evaluation_utils.py
â”‚   â””â”€â”€ command_validator.py
â””â”€â”€ evaluate.py       # Evaluation script
```

## ğŸ“Š Evaluation Metrics

The system uses multiple evaluation metrics:
- Levenshtein distance for command similarity
- Semantic similarity using model embeddings
- Command syntax validation
- LLM-as-a-judge for command quality evaluation using the GPT-4o model and NL2BASH dataset
- Resource usage monitoring (CPU, memory and wall-clock time)
